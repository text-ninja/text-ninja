{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import compute_similarity\n",
    "import time\n",
    "# from preprocess import remove_special_chars, rm_stopwords_stem_lowfreq, fetch_low_freq_words\n",
    "# from tfidf_feature_extraction import calc_freq_distr, calc_idf, calc_tf, calc_tf_idf, compute_para_similarity, compute_similarity_centroid\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import sys\n",
    "\n",
    "EMPTY_INP_SIMILARITY = 'Invalid inputs to the function compute_similarity.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INVALID_INP_TFIDF = 'Error in TF-IDF Calculation. Invalid Input arrays.'\n",
    "INVALID_INP_SIMILARITY = 'Error while computing compute paragraph similarity. Invalid inputs.'\n",
    "EMPTY_INP_FREQ = 'Error while calculating frequency distribution of words. Invalid inputs.'\n",
    "INVALID_INP_IDF = 'Error while calculating IDF. Invalid inputs.'\n",
    "INVALID_INP_TF = 'Error while calculating the Term Frequency. Invalid inputs.'\n",
    "INVALID_INP_CENTRE = 'Error while computing centroid. Invalid inputs.'\n",
    "\n",
    "\"\"\"\n",
    "Calculate the frequency distribution of each word in a paragraph.\n",
    "Outputs an array of the frequency distribution for each paragraph.\n",
    "\"\"\"\n",
    "def calc_freq_distr(input_paragraphs, word_corpus):\n",
    "    \n",
    "    if input_paragraphs.size == 0 or word_corpus.size == 0:\n",
    "        raise ValueError(EMPTY_INP_FREQ)\n",
    "\n",
    "    try:\n",
    "        freq_distribution = np.zeros((len(input_paragraphs), len(word_corpus)))\n",
    "        \n",
    "        for p in range(len(input_paragraphs)):\n",
    "            paragraph = input_paragraphs[p].split()\n",
    "            for w in range(len(word_corpus)):\n",
    "                freq_distribution[p,w] = paragraph.count(word_corpus[w])\n",
    "        return freq_distribution\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error: ', e)\n",
    "        print('Error while calculating frequency distribution of words.')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "calc_idf - Calculate and returns the Inverse Document Frequency score. This function takes frequency_distribution dictionary as input.\n",
    "\n",
    "IDF definition:\n",
    "    IDF(t) = log(N+1/df(t)+1) + 1\n",
    "    where t is each word in the word corpus (feature)\n",
    "    N is the number of paragraphs in the document\n",
    "    df(t) is the count of documents in which the word appears. \n",
    "    \n",
    "    An extra term 1 has been added to numerator and denominator to avoid divide by zero error. \n",
    "    It is equivalent to adding an extra paragraph which contains every word exactly once.\n",
    "\"\"\"\n",
    "def calc_idf(freq_distribution):\n",
    "    \n",
    "    if freq_distribution.size == 0:\n",
    "        raise ValueError(INVALID_INP_IDF)\n",
    "\n",
    "    try:\n",
    "        n_paragraphs, n_words = freq_distribution.shape\n",
    "        word_distribution = np.array([np.count_nonzero(freq_distribution[:,w]) for w in range(n_words)]).reshape(n_words, 1)\n",
    "        doc_count = np.zeros((word_distribution.shape))  + float(n_paragraphs)\n",
    "        idf = np.log(np.divide(1+doc_count,1+word_distribution)).transpose() + 1\n",
    "        return idf\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error: ', e)\n",
    "        print('Error while computing IDF.')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "calc_tf - Calculate and returns the Term Frequency score. This function takes frequency_distribution array as input.\n",
    "TF formula:\n",
    "    TF(t) = Count of each word in the paragraph / Total number of words in the paragraph\n",
    "\"\"\"\n",
    "def calc_tf(freq_distribution):\n",
    "    if freq_distribution.size == 0:\n",
    "        raise ValueError(INVALID_INP_TF)\n",
    "    try:    \n",
    "        word_count = np.repeat(np.sum(freq_distribution, axis = 1).reshape(freq_distribution.shape[0], 1), repeats = freq_distribution.shape[1], axis = 1)\n",
    "        tf = np.divide(freq_distribution, word_count)\n",
    "        return tf\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error: ', e)\n",
    "        print('Error while calculating Term Frequency.')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "calc_tf_idf - Calculate TF-TDF of each word in the document.\n",
    "Inputs:\n",
    "1. Array of Term frequency (TF) of each paragraph in the document\n",
    "2. IDF array of document.\n",
    "Output: \n",
    "Returns a TF-IDF array.\n",
    "\"\"\"\n",
    "def calc_tf_idf(tf, idf):\n",
    "    \n",
    "    if tf.size == 0 or idf.size == 0:\n",
    "        raise ValueError(INVALID_INP_TFIDF)\n",
    "\n",
    "    try:\n",
    "        tf_idf = np.multiply(tf, idf)\n",
    "        norm = np.linalg.norm(tf_idf, axis = 1).reshape(tf.shape[0], 1)\n",
    "        return tf_idf/norm\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error: ', e)\n",
    "        print('Error while calculating TF-IDF.')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "compute_para_similarity - Calculates the similarity of each paragraph.\n",
    "Multiply the TF-IDF vectors of each paragraph with the centroid of the TF-IDF \n",
    "vectors of the paragraph with which you wish to compute the similarity.\n",
    "Input:  array of TF-IDF values of paragraph. \n",
    "Output: Similar paragraphs\n",
    "\"\"\"\n",
    "def compute_para_similarity(tf_idf_array, similar_text, similarity_centroid):\n",
    "    \n",
    "    if tf_idf_array.size == 0 or similarity_centroid.size == 0 or len(similar_text) == 0:\n",
    "        raise ValueError(INVALID_INP_SIMILARITY)\n",
    "    try:\n",
    "        similar_para_idx = []\n",
    "        for i in range(1, tf_idf_array.shape[0]):\n",
    "            if i not in similar_text and np.matmul(similarity_centroid, tf_idf_array[i, :]) > 0.35:  \n",
    "                similar_para_idx.append(i)\n",
    "        return similar_para_idx\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error: ', e)\n",
    "        print('Error while computing paragraph similarity.')\n",
    "\n",
    "\n",
    "def compute_similarity_centroid(tf_idf_array, similar_text):\n",
    "    if tf_idf_array.size == 0 or len(similar_text) == 0:\n",
    "        raise ValueError(INVALID_INP_CENTRE)\n",
    "    try:\n",
    "        similarity_centroid = np.sum(tf_idf_array[similar_text], axis = 0)/len(similar_text)\n",
    "        return similarity_centroid\n",
    "    except Exception as e:\n",
    "        print('Error: ', e)\n",
    "        print('Error while computing centroid.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INVALID_STRING = 'Error while pre-processing the document. Invalid input to the function remove_special_chars.'\n",
    "INVALID_LOW_FREQ = 'Error while pre-processing the document. Passed document does not contain any text.'\n",
    "INVALID_PREPROCESS = 'Error while pre-processing the document. Error in the function rm_stopwords_stem_lowfreq'\n",
    "\n",
    "\"\"\"\n",
    "    remove_special_chars - to remove special characters from a string.    \n",
    "\"\"\"\n",
    "def remove_special_chars(paragraphs):\n",
    "\n",
    "    if len(paragraphs) == 0 or paragraphs is None:\n",
    "        raise ValueError(INVALID_STRING)    \n",
    "    if(True):\n",
    "        clean_string = re.sub(r\"[^a-zA-Z0-9]+\", ' ', paragraphs)\n",
    "        clean_string = re.sub(r'\\s+', ' ', clean_string)\n",
    "        return clean_string.strip()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    get_paragraphs - this function will split the data into sentences\n",
    "\"\"\"\n",
    "def fetch_low_freq_words(paragraphs):\n",
    "\n",
    "    if paragraphs.size == 0:\n",
    "        raise ValueError(INVALID_LOW_FREQ)    \n",
    "\n",
    "    if(True):\n",
    "        paragraphs = np.hstack(np.char.split(paragraphs))\n",
    "        unique, count = np.unique(paragraphs, return_counts=True)\n",
    "        word_count = np.asarray((unique, count)).T\n",
    "        low_freq_words = word_count[np.where(word_count[:,1].astype(int) < 2),0]\n",
    "        return low_freq_words\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    rm_stopwords_stem_lowfreq - to remove the stop words, low frequency words and perform stemming.\n",
    "\"\"\"\n",
    "def rm_stopwords_stem_lowfreq(paragraphs, low_freq_words):\n",
    "\n",
    "    if paragraphs.size == 0:\n",
    "        raise ValueError(INVALID_PREPROCESS)\n",
    "\n",
    "    if(True):\n",
    "        ps = PorterStemmer()\n",
    "#         stop_words = set(stopwords.words('english'))\n",
    "        tokenized_paragraphs = np.char.split(paragraphs)\n",
    "\n",
    "        for p in range(tokenized_paragraphs.shape[0]):\n",
    "            paragraphs[p] = ' '.join([ps.stem(word) for word in tokenized_paragraphs[p] if\n",
    "                            len(word) > 2 and word not in low_freq_words])\n",
    "        return paragraphs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This function is a wrapper which calls the function to pre-process the data.\n",
    "\"\"\"\n",
    "def preprocess_data(paragraph_list):\n",
    "    \n",
    "    \n",
    "    time_in = time.time()\n",
    "    paragraphs = np.array(paragraph_list)\n",
    "\n",
    "    low_freq_words = fetch_low_freq_words(paragraphs)\n",
    "\n",
    "    for p in range(paragraphs.shape[0]):\n",
    "        if len(paragraphs[p]) == 0 or paragraphs[p] is None:\n",
    "            continue;\n",
    "        paragraphs[p] = remove_special_chars(paragraphs[p])\n",
    "    \n",
    "    clean_text = rm_stopwords_stem_lowfreq(paragraphs, low_freq_words)\n",
    "#     print(clean_text)\n",
    "    print('Time taken to pre-process the data: ', time.time() - time_in)\n",
    "    return clean_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function returns the index of the similar paragraphs.\n",
    "\"\"\"\n",
    "def compute_similarity(paragraph_list, similar_text_idx):\n",
    "    if len(paragraph_list) == 0 or len(similar_text_idx) == 0:\n",
    "        raise ValueError(EMPTY_INP_SIMILARITY)\n",
    "    \n",
    "    if(True):\n",
    "        clean_text = preprocess_data(paragraph_list)\n",
    "        print(\"Clean Text : \", clean_text)\n",
    "        word_corpus = np.hstack(np.char.split(clean_text))\n",
    "    \n",
    "        # Calculate Word Frequency in the document\n",
    "        freq_distribution = calc_freq_distr(clean_text, word_corpus)\n",
    "\n",
    "        # Calculate Inverse Document Frequency\n",
    "        word_idf = calc_idf(freq_distribution)\n",
    "        \n",
    "        # Calculate TF term\n",
    "        word_tf = calc_tf(freq_distribution)\n",
    "\n",
    "        # Calculate TF-IDF \n",
    "        word_tf_idf = calc_tf_idf(word_tf, word_idf)\n",
    "\n",
    "        if len(similar_text_idx) > 1:\n",
    "            similarity_centroid = compute_similarity_centroid(word_tf_idf, similar_text_idx)\n",
    "        else:\n",
    "            similarity_centroid = word_tf_idf[similar_text_idx[0]] \n",
    "\n",
    "        # Check Similarity \n",
    "        similar_para_idx = compute_para_similarity(word_tf_idf, similar_text_idx, similarity_centroid)\n",
    "        \n",
    "        return similar_para_idx\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Time taken to pre-process the data:  0.032076358795166016\n",
      "Clean Text :  ['the natur languag process gener the 1950 work can found from period 1950 ture intellig which now the ture intellig'\n",
      " 'the translat more than into english the that year machin translat problem howev wa much slower and the 1966 which found that ten year long research had the expect for machin translat wa reduc research machin translat wa the late when the statist machin translat system were develop'\n",
      " 'some natur languag process system develop the were shrdlu natur languag system restrict block world with restrict vocabulari and eliza psychotherapist written and 1966 inform human emot eliza human like interact the patient the veri base eliza respons for head hurt with whi head hurt'\n",
      " 'dure the 1970 mani conceptu ontolog which world inform into understand data are schank 1975 cullingford 1978 wilenski 1978 meehan 1976 lehnert 1977 carbonel 1979 and lehnert 1981 dure thi mani were written parri racter and jabberwacki'\n",
      " 'the most natur languag process system were base written rule the late howev there wa natur languag process with the machin learn algorithm for languag process wa due the increas see moor law and the the linguist grammar the corpu linguist that the machin learn languag process some the earliest use machin learn algorithm such decis tree produc system hard rule similar exist written rule howev tag the use model natur languag process and research ha focus statist model which make probabilist decis base attach valu weight the the input data the languag model which mani system now are exampl such statist model such model are gener more robust when given unfamiliar input especi input that veri common for world data and produc more reliabl result when into larger system'\n",
      " ''\n",
      " 'mani the earli the machin translat due especi work research where more statist model were develop these system were abl take advantag exist corpora that had been produc the and the for the translat all into all the correspond system govern howev most other system corpora develop for the task these system which wa and often the these system research ha into method more learn from data'\n",
      " ''\n",
      " 'research ha focus and semi learn algorithm such algorithm are abl learn from data that ha not been with the use and non data gener thi task much more difficult than learn and accur result for given amount input data howev there amount non data other thing the the web which can often make for the result the use ha complex practic'\n",
      " ''\n",
      " 'the 2010 learn and deep neural network style machin learn method natur languag process due result that such techniqu can state the art result mani natur languag task for languag pars and mani techniqu the use word word and increas end end learn higher level task answer instead intermedi task tag and pars some area thi ha system are such that deep neural network base approach paradigm from statist natur languag process instanc the neural machin translat nmt the that deep learn base approach machin translat learn sequenc sequenc transform the for intermedi such word and languag that were use statist machin translat smt'\n",
      " ''\n",
      " 'rule base statist the earli day mani languag process system were code set rule write rule for stem howev thi robust natur languag variat'\n",
      " 'the statist the late and 1990 much natur languag process research ha machin learn'\n",
      " ''\n",
      " 'the machin learn paradigm instead for use statist automat learn such rule the larg corpora world exampl corpu plural corpora set document with human annot'\n",
      " ''\n",
      " 'mani differ machin learn algorithm have been natur languag process task these algorithm take input larg set that are from the input data some the earliest use algorithm such decis tree produc system hard rule similar the system written rule that were common increasingli howev research ha focus statist model which make probabilist decis base attach valu weight input featur such model have the advantag that can the mani differ than onli one more reliabl result when such larger system'\n",
      " '' 'system base machin learn algorithm have mani produc rule' ''\n",
      " 'the learn procedur use machin learn automat the most common case when write rule often not all where the direct'\n",
      " 'learn procedur can make use statist algorithm produc model that are robust unfamiliar input word that have not been befor and input with word word omit gener such input with written rule more gener creat system written rule that make decis difficult error prone and consum'\n",
      " 'system base automat learn the rule can made more accur simpli more input data howev system base written rule can onli made more accur the complex the rule which much more difficult task particular there the complex system base craft rule which the system more and more unmanag howev creat more data input machin learn system simpli correspond increas the man hour work gener the complex the process'\n",
      " 'and task the some the most commonli task natur languag process that some these task have world applic more commonli that are use larger task']\n",
      "[2, 6, 10, 13, 15, 17, 21, 23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koshy/miniconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/home/koshy/miniconda3/envs/ml/lib/python3.6/site-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('ak.txt', 'rb') as content_file:\n",
    "    content = content_file.read().decode(errors='replace')\n",
    "    print(len(content.split(\"\\n\")))\n",
    "ids = compute_similarity(content.split(\"\\n\"), [0, 1, 3, 4, 8, 19, 24])\n",
    "\n",
    "data = {}\n",
    "\n",
    "# data[\"index\"] = [0]\n",
    "# data[\"paras\"] = content.split(\"\\n\")\n",
    "# with open('data.json', 'w') as outfile:  \n",
    "#     json.dump(data, outfile)\n",
    "print(ids)\n",
    "# for i in ids:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
